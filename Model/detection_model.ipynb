{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xkhsZubeyaoH"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","\n","def getOpticalFlow(video):\n","    # initialize the list of optical flows\n","    gray_video = []\n","    for i in range(len(video)):\n","        img = cv2.cvtColor(video[i], cv2.COLOR_RGB2GRAY)\n","        gray_video.append(np.reshape(img, (224, 224, 1)))\n","\n","    flows = []\n","    for i in range(0, len(video) - 1):\n","        # calculate optical flow between each pair of frames\n","        flow = cv2.calcOpticalFlowFarneback(gray_video[i], gray_video[i + 1], None, 0.5, 3, 15, 3, 5, 1.2,\n","                                            cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n","        # subtract the mean in order to eliminate the movement of camera\n","        flow[..., 0] -= np.mean(flow[..., 0])\n","        flow[..., 1] -= np.mean(flow[..., 1])\n","        # normalize each component in optical flow\n","        flow[..., 0] = cv2.normalize(flow[..., 0], None, 0, 255, cv2.NORM_MINMAX)\n","        flow[..., 1] = cv2.normalize(flow[..., 1], None, 0, 255, cv2.NORM_MINMAX)\n","        # Add into list\n","        flows.append(flow)\n","\n","    # Padding the last frame as empty array\n","    flows.append(np.zeros((224, 224, 2)))\n","\n","    return np.array(flows, dtype=np.float32)\n","\n","\n","def farneback_visual(flows, file_path):\n","    # visualization farneback optical flow map\n","    # save the map as 'farneback_optical_flow.mp4'\n","    h, w = flows.shape[1:3]\n","\n","    path = '/content/drive/MyDrive/solution_challange/examples'\n","    file_name = '/'.join(file_path.split('.')[0].split('/')[-3:]) + '_opticalFlow'\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out_video = cv2.VideoWriter(os.path.join(path, file_name+'.mp4'), fourcc, 30.0, (int(w), int(h)), isColor=True)\n","\n","    hsv = np.zeros((h, w, 3))\n","    hsv[..., 1] = 255\n","    for flow in flows:\n","        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n","        hsv[...,0] = ang*180/np.pi/2\n","        hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n","        hsv = np.float32(hsv)\n","        rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n","        out_video.write(np.uint8(rgb))\n","    out_video.release()\n","    return None\n","    pass\n","\n","\n","def Video2Npy(file_path, resize=(224,224)):\n","\n","    # Load video\n","    cap = cv2.VideoCapture(file_path)\n","    # Get number of frames\n","    len_frames = int(cap.get(7))\n","    # Extract frames from video\n","    try:\n","        frames = []\n","        for i in range(len_frames-1):\n","            _, frame = cap.read()\n","            frame = cv2.resize(frame,resize, interpolation=cv2.INTER_AREA)\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = np.reshape(frame, (224,224,3))\n","            frames.append(frame)\n","    except:\n","        print(\"Error: \", file_path, len_frames,i)\n","    finally:\n","        frames = np.array(frames)\n","        cap.release()\n","\n","    # Get the optical flow of video\n","    flows = getOpticalFlow(frames)\n","\n","    # Visualize optical flow map\n","    optical_flow_map = farneback_visual(flows, file_path)\n","\n","    result = np.zeros((len(flows),224,224,5))\n","    result[...,:3] = frames\n","    result[...,3:] = flows\n","\n","    return result\n","\n","\n","def Save2Npy(file_dir, save_dir):   # modify this code to save the npy files for your directory or path\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","    # List the files\n","    videos = os.listdir(file_dir)\n","    for v in tqdm(videos):\n","        # Split video name\n","        video_name = v.split('.')[0]\n","        # Get src\n","        video_path = os.path.join(file_dir, v)\n","        # Get dest\n","        save_path = os.path.join(save_dir, video_name+'.npy')\n","        # Load and preprocess video\n","        data = Video2Npy(file_path=video_path, resize=(224,224))\n","        data = np.uint8(data)\n","        # Save as .npy file\n","        np.save(save_path, data)\n","\n","    return None"]},{"cell_type":"markdown","metadata":{"id":"QkHuZFWIyaoK"},"source":["### convert data and save it (offline)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4611134,"status":"ok","timestamp":1706326173715,"user":{"displayName":"김찬원","userId":"02201883091790280641"},"user_tz":-540},"id":"K1KTdmHWyaoM","outputId":"51afdc28-70d7-4c4b-9cd9-2bfd950f5bfd"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 700/700 [1:03:13<00:00,  5.42s/it]\n","100%|██████████| 700/700 [1:01:15<00:00,  5.25s/it]\n","100%|██████████| 100/100 [07:51<00:00,  4.71s/it]\n","100%|██████████| 100/100 [07:45<00:00,  4.65s/it]\n"]}],"source":["source_path = './Data/examples'\n","target_path = './Data/npy_video_ex'\n","\n","for f1 in ['train', 'val']:\n","    for f2 in ['Fight', 'NonFight']:\n","        path1 = os.path.join(source_path, f1, f2)\n","        path2 = os.path.join(target_path, f1, f2)\n","        Save2Npy(file_dir=path1, save_dir=path2)"]},{"cell_type":"markdown","metadata":{"id":"18n6xxd3yaoM"},"source":["## **1. Build Data Loader**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-OlZ4txyaoN"},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import os\n","import cv2\n","\n","class DataGenerator(Dataset):\n","\n","    def __init__(self, directory, data_augmentation=True, phase='train'):\n","        self.phase=phase\n","        self.directory = directory\n","        self.data_aug = data_augmentation\n","        self.X_path, self.Y_dict = self.search_data()\n","        self.print_stats()\n","\n","    def __len__(self):\n","        steps_per_epoch = int(len(self.X_path))\n","        return steps_per_epoch\n","\n","    def __getitem__(self, index):\n","        data, label = self.data_generation(self.X_path[index])\n","        return data.float(), label\n","\n","    def load_data(self, path):\n","        data = np.load(path, mmap_mode='r')\n","        data = self.uniform_sampling(data, target_frames=64)\n","        if self.data_aug:\n","            data[..., :3] = self.color_jitter(data[..., :3])\n","            data = self.random_flip(data, prob=0.5)\n","        data[..., :3] = self.normalize(data[..., :3]) # Normalize RGB\n","        data[..., 3:] = self.normalize(data[..., 3:]) # Normalize optical flows\n","        return data\n","\n","    # shuffle data\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n","\n","    # Normalize data\n","    def normalize(self, data):\n","        mean = data.mean()\n","        std = data.std()\n","        return (data - mean) / std\n","\n","    def random_flip(self, video, prob):\n","        s = np.random.rand()\n","        if s < prob:\n","            video = np.flip(video, (2,)) # Flip in width direction\n","        return video\n","\n","    def uniform_sampling(self, video, target_frames=64):\n","        # get total frames of input video and calculate sampling interval\n","        len_frames = int(len(video))\n","        interval = int(np.ceil(len_frames/target_frames))\n","        # init empty list for sampled video and\n","        sampled_video = []\n","        for i in range(0,len_frames,interval):\n","            sampled_video.append(video[i])\n","        # calculate numer of padded frames and fix it\n","        num_pad = target_frames - len(sampled_video)\n","        padding = []\n","        if num_pad>0:\n","            for i in range(-num_pad,0):\n","                try:\n","                    padding.append(video[i])\n","                except:\n","                    padding.append(video[0])\n","            sampled_video += padding # Add padding results\n","        # get sampled video\n","        return np.array(sampled_video, dtype=np.float32)\n","\n","    # Jitter = spread values\n","    def color_jitter(self, video):\n","        s_jitter = np.random.uniform(-0.2, 0.2)\n","        v_jitter = np.random.uniform(-30, 30)\n","        for i in range(len(video)):\n","            hsv = cv2.cvtColor(np.array(video[i]), cv2.COLOR_RGB2HSV) # Convert RGB -> HSV\n","            s = hsv[..., 1] + s_jitter # saturation jitter\n","            v = hsv[..., 2] + v_jitter # Value jitter\n","            # Flip\n","            s[s < 0] = 0\n","            s[s > 1] = 1\n","            v[v < 0] = 0\n","            v[v > 255] = 255\n","            hsv[..., 1] = s # set jittered saturation\n","            hsv[..., 2] = v # Set jittered value\n","            video[i] = torch.Tensor(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)) # Convert HSV -> RGB again\n","        return video\n","\n","    # print current state sof the Dataset class\n","    def print_stats(self):\n","            self.n_files = len(self.X_path)\n","            self.n_classes = len(self.dirs)\n","            self.indexes = np.arange(len(self.X_path))\n","            np.random.shuffle(self.indexes)\n","            print(\"Found {} files belonging to {} classes.\".format(self.n_files, self.n_classes))\n","            for i, label in enumerate(self.dirs):\n","                print('{:10s} : {}'.format(label, i))\n","\n","    def search_data(self):\n","        X_path = []\n","        Y_dict = {}\n","        self.dirs = sorted(os.listdir(self.directory)) # Get sorted file directories\n","        #one_hots = [0.0, 0.0]\n","        for i, folder in enumerate(self.dirs):\n","            folder_path = os.path.join(self.directory, folder)\n","            for file in os.listdir(folder_path):\n","                file_path = os.path.join(folder_path, file)\n","                # Add file path into X_path\n","                X_path.append(file_path)\n","                # Assign one hot encoded vector into Y_dict\n","                Y_dict[file_path] = i\n","        return X_path, Y_dict\n","\n","    # define batch x using 'load_data' function and batch_y from self.Y_dict.\n","    def data_generation(self, batch_path):\n","        batch_x = np.array(self.load_data(batch_path))\n","        batch_y = np.array(self.Y_dict[batch_path])\n","\n","        batch_x = torch.tensor(batch_x)\n","        batch_y = torch.tensor(batch_y)\n","\n","        return batch_x, batch_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oe_F1RSvyaoO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FusionModel(nn.Module):\n","    def __init__(self):\n","        super(FusionModel, self).__init__()\n","        self.relu=nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        self.maxpool = nn.MaxPool3d(kernel_size=(8, 1 ,1), stride=(8, 1, 1))\n","\n","        # Construct block of RGB layers which takes RGB channel(3) as input\n","        self.rgb_conv1 = nn.Conv3d(3, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.rgb_conv2 = nn.Conv3d(16, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.rgb_maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.rgb_conv3 = nn.Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.rgb_conv4 = nn.Conv3d(16, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.rgb_maxpool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","\n","        self.rgb_conv5 = nn.Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.rgb_conv6 = nn.Conv3d(32, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.rgb_maxpool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.rgb_conv7 = nn.Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.rgb_conv8 = nn.Conv3d(32, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.rgb_maxpool4 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","\n","        # Construct block of optical flow layers which takes the optical flow channel(2) as input\n","        self.opt_conv1 = nn.Conv3d(2, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.opt_conv2 = nn.Conv3d(16, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.opt_maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.opt_conv3 = nn.Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.opt_conv4 = nn.Conv3d(16, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.opt_maxpool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","\n","        self.opt_conv5 = nn.Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.opt_conv6 = nn.Conv3d(32, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.opt_maxpool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.opt_conv7 = nn.Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.opt_conv8 = nn.Conv3d(32, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.opt_maxpool4 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","\n","        # Construct merging Block\n","        self.merge_conv1 = nn.Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.merge_conv2 = nn.Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.merge_maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.merge_conv3 = nn.Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.merge_conv4 = nn.Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.merge_maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","\n","        self.merge_conv5 = nn.Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n","        self.merge_conv6 = nn.Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n","        self.merge_maxpool3 = nn.MaxPool3d(kernel_size=(2, 3, 3), stride=(2, 3, 3))\n","\n","        # Fully Connected Layers\n","        self.fc1 = nn.Linear(128, 128)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc2 = nn.Linear(128, 32)\n","        self.fc3 = nn.Linear(32, 2)\n","\n","        # Initialize weights\n","        self.__init_weight()\n","\n","    def forward(self, x):\n","        x = x.transpose(2, 4)\n","        x = x.transpose(3, 4)\n","        x = x.transpose(1, 2)\n","        rgb = x[:,:3,:,:,:]\n","        opt = x[:,3:5,:,:,:]\n","\n","        # Pass through the RGB data through the blocks of RGB layers\n","        rgb = self.rgb_conv1(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_conv2(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_maxpool1(rgb)\n","        rgb = self.rgb_conv3(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_conv4(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_maxpool2(rgb)\n","\n","        rgb = self.rgb_conv5(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_conv6(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_maxpool3(rgb)\n","        rgb = self.rgb_conv7(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_conv8(rgb)\n","        rgb = self.relu(rgb)\n","        rgb = self.rgb_maxpool4(rgb)\n","\n","        # Pass through the optical flow data through the blocks of RGB layers\n","        opt = self.opt_conv1(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_conv2(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_maxpool1(opt)\n","        opt = self.opt_conv3(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_conv4(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_maxpool2(opt)\n","\n","        opt = self.opt_conv5(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_conv6(opt)\n","        opt = self.relu(opt)\n","        opt = self.opt_maxpool3(opt)\n","        opt = self.opt_conv7(opt)\n","        opt = self.sigmoid(opt)\n","        opt = self.opt_conv8(opt)\n","        opt = self.sigmoid(opt)\n","        opt = self.opt_maxpool4(opt)\n","\n","        # Fuse by performing elementwise multiplication of rgb and opt tensors.\n","        fused = rgb * opt\n","        # Perform maxpooling of fused\n","        fused = self.maxpool(fused)\n","\n","        # Pass through the fused data into merging block\n","        merged = self.merge_conv1(fused)\n","        merged = self.relu(merged)\n","        merged = self.merge_conv2(merged)\n","        merged = self.relu(merged)\n","        merged = self.merge_maxpool1(merged)\n","        merged = self.merge_conv3(merged)\n","        merged = self.relu(merged)\n","        merged = self.merge_conv4(merged)\n","        merged = self.relu(merged)\n","        merged = self.merge_maxpool2(merged)\n","        merged = self.merge_conv5(merged)\n","        merged = self.relu(merged)\n","        merged = self.merge_conv6(merged)\n","        merged = self.relu(merged)\n","        merged = self.merge_maxpool3(merged)\n","\n","        # Fully Connected Layers\n","        x = merged.view(merged.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.fc3(x)\n","\n","        return x\n","\n","    def __init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                # Perform weight initialization (\"kaiming normal\")\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    #nn.init.constant_(m.bias, 0)\n","                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n","                    bound = 1 / torch.sqrt(torch.tensor(fan_in))\n","                    nn.init.uniform_(m.bias, -bound, bound)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17121,"status":"ok","timestamp":1707244085557,"user":{"displayName":"김찬원","userId":"02201883091790280641"},"user_tz":-540},"id":"vYqbo0WGHcOi","outputId":"9bd5d3db-b495-4c00-bd8a-7e19d70cdfe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wandb\n","  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.1-py2.py3-none-any.whl (257 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.8/257.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"]}],"source":["!pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"MFGNwxTgyaoQ"},"source":["##  Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5697,"status":"ok","timestamp":1707244091251,"user":{"displayName":"김찬원","userId":"02201883091790280641"},"user_tz":-540},"id":"Altm6yALyaoR","outputId":"108e2707-c6c3-4223-df62-7ad3a03aefc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 400 files belonging to 2 classes.\n","Fight      : 0\n","NonFight   : 1\n","Found 40 files belonging to 2 classes.\n","Fight      : 0\n","NonFight   : 1\n"]}],"source":["import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from tqdm import tqdm\n","import wandb\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# define model, optimizer and criterion\n","model = FusionModel().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6, nesterov=True)\n","scheduler = StepLR(optimizer, step_size=10, gamma=0.7)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","trainset_path = './Data/npy_video_ex/train'\n","validation_path = './Data/npy_video_ex/val'\n","\n","# define dataset and dataloader\n","train_dataset = DataGenerator(trainset_path, data_augmentation=True, phase='train')\n","val_dataset = DataGenerator(validation_path, data_augmentation=True, phase='val')\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=4)\n","\n","epochs = 10\n","min_loss = np.inf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viedw8e9yaoS"},"outputs":[],"source":["from tqdm.auto import tqdm as tqdm1\n","import torch.nn.functional as F\n","def train():\n","    model.train()\n","    acc_temp = 0\n","    running_loss = 0.0\n","\n","    # forward propagation and backpropagation\n","    # calculate accuracy and loss on training set\n","    for inputs, targets in tqdm1(train_loader, desc='Training'):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        acc_temp += (predicted == targets).sum().item()\n","        running_loss += loss.item()\n","\n","    train_acc = acc_temp / len(train_loader.dataset)\n","    train_loss = running_loss / len(train_loader.dataset)\n","\n","    return train_acc, train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy4gkL6hyaoS"},"outputs":[],"source":["def val():\n","    model.eval()\n","    with torch.no_grad():\n","        running_loss_val = 0\n","        acc_temp_val =0\n","\n","        # calculate accuracy and loss on validation set\n","        for inputs, targets in tqdm1(val_loader, desc='Validation'):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            acc_temp_val += (predicted == targets).sum().item()\n","            running_loss_val += loss.item()\n","\n","        val_acc = acc_temp_val / len(val_loader.dataset)\n","        val_loss = running_loss_val / len(val_loader.dataset)\n","\n","    return val_acc, val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6632,"status":"ok","timestamp":1706748819107,"user":{"displayName":"김찬원","userId":"02201883091790280641"},"user_tz":-540},"id":"EinE9sufJr9a","outputId":"41dc3564-4805-4e20-c2cc-4d57a7f4a62e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login --relogin"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226,"referenced_widgets":["d6aba6d140dd4e3a95255b79bfb49edc","bbd87813616a424dbdb3a6b39464cfff","7a890539f12549fd850d7bbf27d1d9f5","3e0537ea1a9a4ae5911823cd19e7b3bb","a6fc7563ceb5412ab2f5b1238f7b7d9f","9786867176054cc78d0d52dc122a04ce","5d443adcdb8147cf880eb12197ae2c81","b9f256c678104ff99193e9970939c1ba","4f117530366f48b187550e03b3007a4d","e92e0662cc1d433f9b7631eb83afd921","8769b400d2d84c3aa1d66c5b132cd272"]},"id":"59a0A9layaoS","outputId":"7daaff9a-daa9-4794-8b0d-6beb60aee54d"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchanwon0721\u001b[0m (\u001b[33mcompute_vision\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.16.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20240206_182820-pfxpmeyk</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/compute_vision/solution_challange/runs/pfxpmeyk' target=\"_blank\">1</a></strong> to <a href='https://wandb.ai/compute_vision/solution_challange' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/compute_vision/solution_challange' target=\"_blank\">https://wandb.ai/compute_vision/solution_challange</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/compute_vision/solution_challange/runs/pfxpmeyk' target=\"_blank\">https://wandb.ai/compute_vision/solution_challange/runs/pfxpmeyk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6aba6d140dd4e3a95255b79bfb49edc","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import random\n","import numpy as np\n","\n","seed=0\n","random.seed(seed)\n","np.random.seed(seed)\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","torch.manual_seed(seed)\n","if device =='cuda':\n","    print('gpu device is using')\n","    torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic=True\n","torch.backends.cudnn.benchmark=False\n","\n","wandb.login()\n","wandb.init(project='solution_challange', name='1')\n","\n","for epoch in tqdm(range(epochs)):\n","    # Perform training and validation\n","    # Save the weights\n","    # Log the training and validation results\n","    epoch_acc, epoch_loss = train()\n","    epoch_acc_val, epoch_loss_val = val()\n","\n","    print(f'Epoch: {epoch}, Train Accuracy: {epoch_acc}, Train Loss: {epoch_loss}')\n","    print(f'Epoch: {epoch}, Valid Accuracy: {epoch_acc_val}, Valid Loss: {epoch_loss_val}')\n","    wandb.log({'Epoch':epoch, 'Train Accuracy':epoch_acc, \"Train Loss\":epoch_loss})\n","    wandb.log({'Epoch':epoch, 'Valid Accuracy':epoch_acc_val, \"Valid Loss\":epoch_loss_val})\n","\n","    if epoch_loss_val <= min_loss:\n","        min_loss = epoch_loss_val\n","        torch.save(model.state_dict(), './Output/model_weight3.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110875,"status":"ok","timestamp":1706754703908,"user":{"displayName":"김찬원","userId":"02201883091790280641"},"user_tz":-540},"id":"17TMyJKlEyVR","outputId":"60ff20be-b94c-464d-cbde-a6fc26d43519"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [01:00<00:00,  6.03s/it]\n","100%|██████████| 10/10 [00:49<00:00,  4.97s/it]\n"]}],"source":["source_path = './Data/examples'\n","target_path = './Data/npy_video_ex'\n","\n","for f1 in ['test']:\n","    for f2 in ['Fight', 'NonFight']:\n","        path1 = os.path.join(source_path, f1, f2)\n","        path2 = os.path.join(target_path, f1, f2)\n","        Save2Npy(file_dir=path1, save_dir=path2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76MXD-sIqywQ"},"outputs":[],"source":["best_model = './Data/model_weight3.pth'\n","test_path = './Data/npy_video_ex/test'\n","\n","test_dataset = DataGenerator(directory=test_path, data_augmentation=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esIZtemmyaoT"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","\n","model.load_state_dict(torch.load(best_model))\n","model.eval()\n","with torch.no_grad():\n","    # calculate accuracy and AUROC on test set\n","    running_loss_test = 0\n","    acc_temp_test =0\n","\n","    preds = []\n","    targets = []\n","    for input, target in tqdm(test_loader, desc='Test'):\n","        input, target = input.to(device), target.to(device)\n","\n","        output = model(input)\n","        loss = loss_fn(output, target)\n","\n","        _, predicted = torch.max(output, 1)\n","        acc_temp_test += (predicted == target).sum().item()\n","        running_loss_test += loss.item()\n","\n","        preds.append(predicted.cpu().numpy())\n","        targets.append(target.cpu().numpy())\n","\n","    test_acc = acc_temp_test / len(test_loader.dataset)\n","    test_loss = running_loss_test / len(test_loader.dataset)\n","    preds = np.array(preds).flatten()\n","    targets = np.array(targets).flatten()\n","\n","print(f'test accuracy: {test_acc}')\n","print(f'AUROC: {roc_auc_score(targets, preds)}')\n","\n","fpr, tpr, thresholds = roc_curve(targets, preds)\n","plt.plot(fpr, tpr)\n","plt.xlabel('fpr')\n","plt.ylabel('tpr')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHoNeR1gZ5SJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3e0537ea1a9a4ae5911823cd19e7b3bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e92e0662cc1d433f9b7631eb83afd921","placeholder":"​","style":"IPY_MODEL_8769b400d2d84c3aa1d66c5b132cd272","value":" 0/100 [00:00&lt;?, ?it/s]"}},"4f117530366f48b187550e03b3007a4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d443adcdb8147cf880eb12197ae2c81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a890539f12549fd850d7bbf27d1d9f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f256c678104ff99193e9970939c1ba","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f117530366f48b187550e03b3007a4d","value":0}},"8769b400d2d84c3aa1d66c5b132cd272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9786867176054cc78d0d52dc122a04ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6fc7563ceb5412ab2f5b1238f7b7d9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f256c678104ff99193e9970939c1ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbd87813616a424dbdb3a6b39464cfff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9786867176054cc78d0d52dc122a04ce","placeholder":"​","style":"IPY_MODEL_5d443adcdb8147cf880eb12197ae2c81","value":"Training:   0%"}},"d6aba6d140dd4e3a95255b79bfb49edc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbd87813616a424dbdb3a6b39464cfff","IPY_MODEL_7a890539f12549fd850d7bbf27d1d9f5","IPY_MODEL_3e0537ea1a9a4ae5911823cd19e7b3bb"],"layout":"IPY_MODEL_a6fc7563ceb5412ab2f5b1238f7b7d9f"}},"e92e0662cc1d433f9b7631eb83afd921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
